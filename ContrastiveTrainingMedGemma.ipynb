{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["8OFBMa5RDCPR","Y0zBqJaWDH75","a0TSBkTIDQ5t","GppY17_xOdZp","VUJNNsdUNxAR","TudAolK2K7Q_","dBhOr_LUOpix","Sgk3gCQtRiXq","Bx9ZrqVtYpsQ","LPb-UPCS-W0T","zivCvFK1rbmC","S5VkpRwrew8Q","3BZ6ESULxFQR","Twko3nAqxffg","fAdZQXKfxcCM","gnjgiUMVx4-Z","GqCE4DlUyJzE","biftkfM5yMTS"],"gpuType":"A100","machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Install, Import, and Mount Drive"],"metadata":{"id":"8OFBMa5RDCPR"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"15OxH9CK2ED8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["! pip install --upgrade --quiet bitsandbytes datasets peft transformers python-docx"],"metadata":{"collapsed":true,"id":"oS3j-6bh5Bba"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import re\n","import json\n","import PIL\n","\n","from typing import Any, Dict, Literal\n","from collections import defaultdict\n","from PIL import Image as PILImage\n","from tqdm import tqdm\n","from docx import Document\n","\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision.transforms as T\n","from transformers import (\n","    AutoProcessor,\n","    PaliGemmaForConditionalGeneration, AutoModelForImageTextToText,\n","    BitsAndBytesConfig, get_constant_schedule_with_warmup\n",")\n","from datasets import load_dataset, concatenate_datasets, Image\n","from peft import get_peft_model, LoraConfig, TaskType\n","import wandb\n"],"metadata":{"id":"u516y6j7N2Wk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Preprocess Data and Save to Drive\n","Skip this section, if next section (Load Data from Drive) works without issue"],"metadata":{"id":"Y0zBqJaWDH75"}},{"cell_type":"code","source":["# Function to parse text report into dict, easier for formating later\n","def parse_doc_into_dict(document_obj):\n","  document = [paragraph.text for paragraph in document_obj.paragraphs]\n","  document_string = '\\n'.join(document)\n","\n","  patient_id = re.search(r'PATIENT NO\\.(.*)',document_string)\n","  acr_category = re.search(r\"(ACR.*)\", document_string)\n","  right_breast_first_finding = re.search(r\"Right Breast:(.*?)\\n\\n\", document_string, flags=re.DOTALL)\n","  left_breast_first_finding = re.search(r\"Left Breast:(.*?)\\n\\n\", document_string, flags=re.DOTALL)\n","  right_breast_opinion = re.search(r\"OPINION:.*?Right Breast:(.*?)\\n\\n\", document_string, flags=re.DOTALL)\n","  left_breast_opinion = re.search(r\"OPINION:.*?Left Breast:(.*?)\\n\\n\", document_string, flags=re.DOTALL)\n","  right_breast_second_finding = re.search(r\"CONTRAST ENHANCED SPECTRAL.*?Right Breast:\\n{1,2}(.*?)(\\n\\n|$)\", document_string, flags=re.DOTALL)\n","  left_breast_second_finding = re.search(r\"CONTRAST ENHANCED SPECTRAL.*?Left Breast:\\n{1,2}(.*?)(\\n\\n|$)\", document_string, flags=re.DOTALL)\n","\n","  return {\n","      'patient_id': patient_id.group(1).strip() if patient_id else \"\",\n","      'acr_category': acr_category.group(1).strip() if acr_category else \"\",\n","      'right_breast_first_finding': right_breast_first_finding.group(1).strip() if right_breast_first_finding else \"\",\n","      'left_breast_first_finding': left_breast_first_finding.group(1).strip() if left_breast_first_finding else \"\",\n","      'right_breast_opinion': right_breast_opinion.group(1).strip() if right_breast_opinion else \"\",\n","      'left_breast_opinion': left_breast_opinion.group(1).strip() if left_breast_opinion else \"\",\n","      'right_breast_second_finding': right_breast_second_finding.group(1).strip() if right_breast_second_finding else \"\",\n","      'left_breast_second_finding': left_breast_second_finding.group(1).strip() if left_breast_second_finding else \"\",\n","  }\n","\n","# Parse all report and saved into a list of dictionary\n","report_root_dir = \"/content/drive/MyDrive/Dataset/Medical-reports-for-cases-/Medical reports for cases\"\n","files = [os.path.join(report_root_dir, f) for f in os.listdir(report_root_dir) if f.lower().endswith('.docx')]\n","\n","\n","# parsed_report: Dict[patient_id, patient_info]\n","# patient_info: Dict[data_key, data_value]\n","parsed_reports: Dict[str, Dict[str, str]] = {}\n","\n","for file in tqdm(files):\n","    document_obj = Document(file)\n","    document_dict = parse_doc_into_dict(document_obj)\n","    patient_id = document_dict.pop('patient_id')\n","    parsed_reports[patient_id] = document_dict\n","\n","# Example\n","parsed_reports.get('176')"],"metadata":{"id":"XTymAzv3488C"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Extract only portion of the report relevant to the image and format it into natural language strings\n","def extract_and_generate_report(left_or_right: Literal[\"L\",\"R\"],\n","                                mammography_type: Literal[\"CM\", \"DM\"],\n","                                report_dict: dict[str, str]) -> str:\n","\n","    acr_line = f\"{report_dict['acr_category'] if mammography_type == 'DM' else ''}\" # Only include ACR information for Low energy image\n","\n","    findings_line = \"\"\n","\n","    if left_or_right == \"R\" and mammography_type == \"DM\":\n","        findings_line = f\"Findings: \\n{report_dict['right_breast_first_finding']}\\n\\nOpinion: \\n{report_dict['right_breast_opinion']}\"\n","    elif left_or_right == \"R\" and mammography_type == \"CM\":\n","        findings_line = f\"Findings: \\n{report_dict['right_breast_second_finding']}\"\n","    elif left_or_right == \"L\" and mammography_type == \"DM\":\n","        findings_line = f\"Findings: \\n{report_dict['left_breast_first_finding']}\\n\\nOpinion: \\n{report_dict['left_breast_opinion']}\"\n","    elif left_or_right == \"L\" and mammography_type == \"CM\":\n","        findings_line = f\"Findings: \\n{report_dict['left_breast_second_finding']}\"\n","\n","    generated_report = f\"{acr_line}\\n\\n{findings_line}\".strip()\n","\n","    return generated_report\n","\n","# Example\n","print(extract_and_generate_report(\"L\", \"CM\", parsed_reports.get('176')))"],"metadata":{"id":"tGvnRgWM7ZP3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["images_root_dir = '/content/drive/MyDrive/CDD-CESM-curated-dataset/images'\n","\n","# Group images by patient_id\n","patient_data = defaultdict(lambda: {\n","    \"patient_id\": None,\n","    \"image_l_cc_dm\": None,\n","    \"image_l_cc_cm\": None,\n","    \"image_l_mlo_dm\": None,\n","    \"image_l_mlo_cm\": None,\n","    \"image_r_cc_dm\": None,\n","    \"image_r_cc_cm\": None,\n","    \"image_r_mlo_dm\": None,\n","    \"image_r_mlo_cm\": None,\n","    \"report_l_dm\": None,\n","    \"report_r_dm\": None,\n","    \"report_l_cm\": None,\n","    \"report_r_cm\": None,\n","})\n","\n","for image_file_name in tqdm(os.listdir(images_root_dir)):\n","    if not image_file_name.lower().endswith('.jpg'):\n","        continue\n","\n","    # Extract metadata\n","    patient_id = re.search(r\"P(.*?)_\", image_file_name).group(1)\n","    left_or_right = re.search(r\".*?_([LR])_\", image_file_name).group(1)\n","    cc_or_mlo = re.search(r\".*?_(CC|MLO)\", image_file_name).group(1)\n","    mammography_type = re.search(r\".*?_(CM|DM)_\", image_file_name).group(1)\n","\n","    # Build keys\n","    image_key = f\"image_{left_or_right}_{cc_or_mlo}_{mammography_type}\".lower()\n","    report_key = f\"report_{left_or_right}_{mammography_type}\".lower()\n","\n","    # Path data & report data\n","    image_full_path = os.path.join(images_root_dir, image_file_name)\n","    report_dict = parsed_reports.get(patient_id)\n","    report = extract_and_generate_report(left_or_right, mammography_type, report_dict)\n","\n","    # Store in patient data\n","    patient_data[patient_id][\"patient_id\"] = patient_id\n","    patient_data[patient_id][image_key] = image_full_path\n","    patient_data[patient_id][report_key] = report\n","\n","# Example\n","patient_data.get('176')"],"metadata":{"collapsed":true,"id":"Ww0aMc35xt1M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert to list of dictionaries to write into jsonl files and save it\n","datasets = list(patient_data.values())\n","output_file = \"medgemma_contrastive_dataset.jsonl\"\n","\n","with open(output_file, 'w', encoding='utf-8') as f:\n","  for json_line in datasets:\n","    f.write(json.dumps(json_line) + '\\n')\n","\n","!cp /content/medgemma_contrastive_dataset.jsonl /content/drive/MyDrive/medgemma_contrastive_dataset.jsonl"],"metadata":{"id":"6QzYSX5yzgaN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Load Data from Drive"],"metadata":{"id":"a0TSBkTIDQ5t"}},{"cell_type":"code","source":["train_size = 0.9  # @param {type: \"number\"}\n","validation_size = 0.1  # @param {type: \"number\"}\n","\n","dataset = load_dataset(\"json\", data_files=\"/content/drive/MyDrive/medgemma_contrastive_dataset.jsonl\", split=\"train\")\n","dataset = dataset.cast_column(\"image_l_cc_dm\", Image())\n","dataset = dataset.cast_column(\"image_l_cc_cm\", Image())\n","dataset = dataset.cast_column(\"image_l_mlo_dm\", Image())\n","dataset = dataset.cast_column(\"image_l_mlo_cm\", Image())\n","dataset = dataset.cast_column(\"image_r_cc_dm\", Image())\n","dataset = dataset.cast_column(\"image_r_cc_cm\", Image())\n","dataset = dataset.cast_column(\"image_r_mlo_dm\", Image())\n","dataset = dataset.cast_column(\"image_r_mlo_cm\", Image())\n"],"metadata":{"collapsed":true,"id":"xKs1l62PB23d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Process to jsonl file\n","\n"],"metadata":{"id":"1JM2shnda2K8"}},{"cell_type":"markdown","source":["This step is to prepare for custom training data loop in the contrastive learning step.   \n","Skip the next cell and use `dataset` if using it in HuggingFace Trainer loop\n"],"metadata":{"id":"9I_JNCpecPLd"}},{"cell_type":"code","source":["train_size = 0.9  # @param {type: \"number\"}\n","validation_size = 0.1  # @param {type: \"number\"}\n","\n","dataset = load_dataset(\"json\", data_files=\"/content/drive/MyDrive/medgemma_contrastive_dataset.jsonl\", split=\"train\")\n","\n","dataset_l = (dataset\n","    .select_columns([\"image_l_cc_dm\", \"image_l_mlo_dm\", \"report_l_dm\"])\n","    .rename_column(\"image_l_cc_dm\", \"cc_path\")\n","    .rename_column(\"image_l_mlo_dm\", \"mlo_path\")\n","    .rename_column(\"report_l_dm\", \"report\"))\n","\n","dataset_r = (dataset\n","    .select_columns([\"image_r_cc_dm\", \"image_r_mlo_dm\", \"report_r_dm\"])\n","    .rename_column(\"image_r_cc_dm\", \"cc_path\")\n","    .rename_column(\"image_r_mlo_dm\", \"mlo_path\")\n","    .rename_column(\"report_r_dm\", \"report\"))\n","\n","combined_dataset = concatenate_datasets([dataset_l, dataset_r])\n","filtered_dataset = combined_dataset.filter(\n","    lambda x: x['cc_path'] and\n","              x['mlo_path'] and\n","              x['report']\n",")\n","\n","data = filtered_dataset.train_test_split(\n","    train_size=train_size,\n","    test_size=validation_size,\n","    shuffle=True,\n","    seed=42,\n",")\n","\n","data[\"validation\"] = data.pop(\"test\") # Change test into validation\n","print(f\"Train split size: {len(data['train'])}\\nValidation split size: {len(data['validation'])}\")\n","data"],"metadata":{"id":"glLdLGtuElC2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data[\"train\"].to_json(\"train_dataset.jsonl\", orient=\"records\", lines=True)\n","data[\"validation\"].to_json(\"validation_dataset.jsonl\", orient=\"records\", lines=True)"],"metadata":{"id":"ZRmOYCDRWJbQ","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training setup"],"metadata":{"id":"fFu9U7h4OX9t"}},{"cell_type":"markdown","source":["## Config"],"metadata":{"id":"GppY17_xOdZp"}},{"cell_type":"code","source":["class Config:\n","    model_id = \"google/medgemma-4b-it\"\n","    train_file = \"train_dataset.jsonl\"\n","    validation_file = \"validation_dataset.jsonl\"\n","    output_dir = \"./mammo_clip_checkpoints\"\n","\n","    # Hyperparameters\n","    max_length = 128          # Text length for reports\n","    batch_size = 32           # Keep small per GPU\n","    grad_accumulation = 1     # Useless, no need to use\n","    num_epochs = 50           # Recommend 25~50 Maybe stop at 10 to prevent overfitting\n","    learning_rate = 2e-4      # LoRA needs higher learning rate (other papers use 5e-5 without LoRA)\n","    weight_decay = 0.05 #5e-3       # Smaller dataset needs higher weight decay to prevent overfit (Other paper uses 1e-4, some highly similar paper use 0.05)\n","    temperature = 0.07        # Learnable or fixed for InfoNCE loss\n","    projection_dim = 512      # Dimension to project images/text into\n","    val_split_pct = 0.1       # 10% for validation\n","    num_workers = 2           # Simple parameter to speed up data loading\n","    lora_r = 8                # LoRA r parameter\n","    lora_alpha = 16           # LoRA alpha parameter\n","    lora_dropout = 0.40       # LoRA dropout parameter"],"metadata":{"id":"lAlm0exjOaU5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Dataset"],"metadata":{"id":"VUJNNsdUNxAR"}},{"cell_type":"code","source":["class MammoDataset(Dataset):\n","    def __init__(self, jsonl_file, processor, split=\"test\"): # Default is test split to bypass data augmentation pipeline\n","        self.data = []\n","        with open(jsonl_file, 'r') as f:\n","            for line in f:\n","                self.data.append(json.loads(line))\n","        self.processor = processor\n","\n","        # Data augmentation pipeline for training\n","        self.split = split\n","        self.transform = T.Compose([\n","                # 1. Random Resized Crop:\n","                T.RandomResizedCrop(size=(896, 896), scale=(0.7, 0.9), ratio=(0.45, 0.65)),\n","\n","                # 2. Intensity/Contrast Jitter\n","                T.ColorJitter(brightness=0.2, contrast=0.2),\n","\n","                # 3. Random Gaussian Blur\n","                T.RandomApply([T.GaussianBlur(kernel_size=19, sigma=(0.1, 2.0))], p=0.5)\n","            ])\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        item = self.data[idx]\n","\n","        # Load Images\n","        image_cc = PILImage.open(item['cc_path']).convert(\"RGB\")\n","        image_mlo = PILImage.open(item['mlo_path']).convert(\"RGB\")\n","\n","        if self.split == \"train\":\n","            image_cc = self.transform(image_cc)\n","            image_mlo = self.transform(image_mlo)\n","\n","        text = item['report']\n","\n","        # Process Inputs using the VLM processor\n","        # We process CC and MLO separately but using the same pipeline\n","        inputs_cc = self.processor.image_processor(images=image_cc, return_tensors=\"pt\")\n","        inputs_mlo = self.processor.image_processor(images=image_mlo, return_tensors=\"pt\")\n","        report = self.processor.tokenizer(text, return_tensors=\"pt\", padding=\"max_length\", max_length=Config.max_length, truncation=True)\n","\n","\n","        return {\n","            \"pixel_values_cc\": inputs_cc[\"pixel_values\"].squeeze(0),\n","            \"pixel_values_mlo\": inputs_mlo[\"pixel_values\"].squeeze(0),\n","            \"input_ids\": report[\"input_ids\"].squeeze(0), # Text is same for both\n","            \"attention_mask\": report[\"attention_mask\"].squeeze(0)\n","        }"],"metadata":{"id":"vtPs2gxCOhZM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model Architecture"],"metadata":{"id":"TudAolK2K7Q_"}},{"cell_type":"code","source":["class MammoContrastiveModel(nn.Module):\n","    def __init__(self, base_model, hidden_size, projection_dim=512):\n","        super().__init__()\n","        self.base_model = base_model\n","\n","        # Contrastive Projection Heads\n","        # We project the large VLM embeddings down to a smaller \"CLIP space\"\n","        self.visual_projection = nn.Linear(1152, projection_dim).to(dtype=torch.bfloat16) # SigLIP embedding size (usually 1152)\n","        self.text_projection = nn.Linear(hidden_size, projection_dim).to(dtype=torch.bfloat16) # LLM hidden size\n","        self.logit_scale = nn.Parameter(torch.ones([]) * 2.6592) # ln(1/0.07)\n","\n","    def get_image_features(self, pixel_values):\n","        # Extract features from the Vision Tower (SigLIP) directly\n","        # Note: Accessing .vision_tower depends on specific HF implementation of MedGemma/PaliGemma\n","        vision_outputs = self.base_model.vision_tower(pixel_values)\n","\n","        # SigLIP usually outputs (Batch, Num_Patches, Dim)\n","        # We pool these features. Global Average Pooling is standard for CLIP.\n","        pooled_vision = vision_outputs.last_hidden_state.mean(dim=1)\n","\n","        # Cast to the projection layer's dtype\n","        return self.visual_projection(pooled_vision.to(self.visual_projection.weight.dtype))\n","\n","    def get_text_features(self, input_ids, attention_mask):\n","        # Extract features from the LLM (Gemma)\n","        # We pass text through the LLM and take the LAST token (EOS) as the representation\n","        outputs = self.base_model.language_model(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            output_hidden_states=True\n","        )\n","        last_hidden_state = outputs.hidden_states[-1]\n","        sequence_lengths = attention_mask.sum(dim=1) - 1\n","        batch_size = last_hidden_state.shape[0]\n","        pooled_text = last_hidden_state[torch.arange(batch_size, device=last_hidden_state.device), sequence_lengths]\n","\n","        # Cast to the projection layer's dtype\n","        return self.text_projection(pooled_text.to(self.text_projection.weight.dtype))\n","\n","    def forward(self, pixel_values_cc, pixel_values_mlo, input_ids, attention_mask):\n","        # 1. Get Embeddings\n","        z_cc = self.get_image_features(pixel_values_cc)\n","        z_mlo = self.get_image_features(pixel_values_mlo)\n","        z_text = self.get_text_features(input_ids, attention_mask)\n","\n","        # 2. Normalize\n","        z_cc = F.normalize(z_cc, p=2, dim=1)\n","        z_mlo = F.normalize(z_mlo, p=2, dim=1)\n","        z_text = F.normalize(z_text, p=2, dim=1)\n","\n","        return z_cc, z_mlo, z_text"],"metadata":{"id":"3Tboab0vOnia"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Proposed Contrastive Loss Function"],"metadata":{"id":"dBhOr_LUOpix"}},{"cell_type":"code","source":["def contrastive_loss(z1, z2, logit_scale):\n","    \"\"\"\n","    Standard CLIP Loss: Symmetric Cross Entropy\n","    \"\"\"\n","    batch_size = z1.size(0)\n","\n","    # Cosine similarity\n","    cosine_similarity = torch.matmul(z1, z2.t())\n","\n","    logits_per_1 = cosine_similarity * logit_scale.exp()\n","    logits_per_2 = logits_per_1.t()\n","\n","    labels = torch.arange(batch_size, device=z1.device)\n","\n","    loss_1 = F.cross_entropy(logits_per_1, labels)\n","    loss_2 = F.cross_entropy(logits_per_2, labels)\n","\n","    loss = (loss_1 + loss_2) / 2 # For training / optimization objectives\n","    total_consine_similarity = cosine_similarity.diag().sum() # Only for reporting purposes\n","\n","    return loss, total_consine_similarity"],"metadata":{"id":"mI6_iQeqOn1L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Model Initialization"],"metadata":{"id":"Sgk3gCQtRiXq"}},{"cell_type":"code","source":["# A. Load Model in 4-bit (QLoRA)\n","model_kwargs = dict(\n","    attn_implementation=\"sdpa\", # \"sdpa\", \"eager\"\n","    torch_dtype=torch.bfloat16,\n","    device_map=\"auto\",\n",")\n","\n","model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=model_kwargs[\"torch_dtype\"],\n","    bnb_4bit_quant_storage=model_kwargs[\"torch_dtype\"],\n",")\n","\n","gemma_model = AutoModelForImageTextToText.from_pretrained(Config.model_id, **model_kwargs)\n","processor = AutoProcessor.from_pretrained(Config.model_id)\n","\n","# Use right padding to avoid issues during training\n","processor.tokenizer.padding_side = \"right\"\n","\n","\n","# Disable cache for gradient checkpointing\n","if hasattr(gemma_model.config, \"use_cache\"):\n","    gemma_model.config.use_cache = False\n","\n","# Enable Gradient Checkpointing (save memory and allow larger batch size)\n","gemma_model.gradient_checkpointing_enable()\n","\n","# Fix for \"None of the inputs have requires_grad=True\" warning\n","# This ensures gradients flow through, so checkpointing works\n","gemma_model.enable_input_require_grads()"],"metadata":{"collapsed":true,"id":"CZn0S8w8Reuc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# B. Apply LoRA (Crucial for training 4B model on limited hardware)\n","# We target both vision and language modules\n","peft_config = LoraConfig(\n","    r=Config.lora_r,\n","    lora_alpha=Config.lora_alpha,\n","    lora_dropout=Config.lora_dropout,\n","    bias=\"none\",\n","    task_type=\"FEATURE_EXTRACTION\", # Custom task effectively\n","\n","    #target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n","    target_modules=\"all-linear\",\n","    # modules_to_save=[\n","    #     \"lm_head\",\n","    #     \"embed_tokens\",\n","    # ],\n","\n",")\n","base_model = get_peft_model(gemma_model, peft_config)\n","base_model.print_trainable_parameters()\n","\n","# C. Wrap in our Contrastive Module\n","# Note: We cast base model to float32 for stable contrastive training if memory allows,\n","# or keep in mixed precision.\n","model = MammoContrastiveModel(base_model, hidden_size=base_model.config.text_config.hidden_size).cuda()"],"metadata":{"id":"aFYncEZQTWYe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Optimizer and DataLoader"],"metadata":{"id":"Bx9ZrqVtYpsQ"}},{"cell_type":"code","source":["# D. Optimizer\n","# We only train the LoRA adapters and the new Projection Heads\n","optimizer = torch.optim.AdamW(model.parameters(), lr=Config.learning_rate, weight_decay=Config.weight_decay)\n","\n","# E. Data Loader\n","dataset = MammoDataset(Config.train_file, processor, split=\"train\")\n","dataloader = DataLoader(dataset, batch_size=Config.batch_size, shuffle=True, num_workers=Config.num_workers)\n","\n","validation_dataset = MammoDataset(Config.validation_file, processor, split=\"test\")\n","validation_dataloader = DataLoader(validation_dataset, batch_size=Config.batch_size, shuffle=False, num_workers=Config.num_workers)\n","\n","# F. Scheduler\n","num_update_steps_per_epoch = len(dataloader) // Config.grad_accumulation\n","max_train_steps = Config.num_epochs * num_update_steps_per_epoch\n","\n","lr_scheduler = get_constant_schedule_with_warmup(\n","    optimizer=optimizer,\n","    num_warmup_steps=int(0.1 * max_train_steps), # 10% Warmup\n","    # num_training_steps=max_train_steps\n",")"],"metadata":{"collapsed":true,"id":"LTU6HBWmWgkV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Validation function"],"metadata":{"id":"LPb-UPCS-W0T"}},{"cell_type":"code","source":["# Recall at K (R@k) metrics\n","def compute_recall_k(z_img, z_text, k_vals=[1,2,3]):\n","    \"\"\"\n","    Computes R@1, R@5, R@10 given normalized image and text embeddings.\n","    \"\"\"\n","    # 1. Calculate similarity matrix (Dot product of normalized vectors = Cosine Similarity)\n","    # Shape: [N_samples, N_samples]\n","    logits = torch.matmul(z_img, z_text.t())\n","\n","    # 2. Ground truth: The i-th image matches the i-th text (diagonal)\n","    batch_size = logits.shape[0]\n","    labels = torch.arange(batch_size, device=logits.device)\n","\n","    results = {}\n","\n","    # 3. Find top K matches\n","    max_k = max(k_vals)\n","    _, top_indices = logits.topk(max_k, dim=1)\n","\n","    for k in k_vals:\n","        # Check if the correct label is within the top k predictions\n","        is_correct = top_indices[:, :k].eq(labels.unsqueeze(1)).any(dim=1)\n","        results[f\"R@{k}\"] = is_correct.float().mean().item() * 100\n","\n","    return results"],"metadata":{"id":"PnaJBiio-Up4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## WandB Logger Initialization"],"metadata":{"id":"zivCvFK1rbmC"}},{"cell_type":"code","source":["from google.colab import userdata\n","os.environ[\"WANDB_API_KEY\"] = userdata.get('WANDB_API_KEY')\n","\n","config_dict = {k: v for k, v in vars(Config).items() if not k.startswith('_')}\n","config_dict\n","\n","logger = wandb.init(\n","    entity=\"wjlingz-none\",\n","    project=\"mammogram-contrastive-learning-project\",\n","    name=\"contrastive-training\",\n","    config=config_dict\n",")\n","\n","# Use `logger.log({\"acc\": 0.1, \"loss\": 0.2})` to log"],"metadata":{"id":"vlXwXvc2reo_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training Loop"],"metadata":{"id":"_it2uVV3OxX1"}},{"cell_type":"code","source":["def train():\n","\n","    print(\"Starting Contrastive Pre-Training...\")\n","\n","    best_eval_loss = float('inf') # Used for early stopping, use high loss as a starting point\n","    best_eval_loss_epoch = 0 # Early stopping if loss is not improving for 3 consecutive epochs\n","\n","    for epoch in range(Config.num_epochs):\n","\n","        optimizer.zero_grad()\n","        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}\")\n","        num_steps = len(dataloader)\n","        training_data_size = len(dataset)\n","        validation_data_size = len(validation_dataset)\n","\n","        # ========== Training loop ==========\n","        model.train()     # Switch to training mode\n","        epoch_loss = 0      # For training performance evaluation\n","        all_training_z_cc = []\n","        all_training_z_mlo = []\n","        all_training_z_text = []\n","        cc_text_total_cos_sim = 0\n","        mlo_text_total_cos_sim = 0\n","        cc_mlo_total_cos_sim = 0\n","\n","        for step, batch in enumerate(progress_bar):\n","            # Move to GPU\n","            # Use bfloat16 to match the model weights\n","            pv_cc = batch[\"pixel_values_cc\"].cuda().to(torch.bfloat16).requires_grad_(True)\n","            pv_mlo = batch[\"pixel_values_mlo\"].cuda().to(torch.bfloat16).requires_grad_(True)\n","            ids = batch[\"input_ids\"].cuda()\n","            mask = batch[\"attention_mask\"].cuda()\n","\n","            # Forward\n","            z_cc, z_mlo, z_text = model(pv_cc, pv_mlo, ids, mask)\n","\n","            # Calculate Losses\n","            loss_cc_text, cc_text_summed_cos_sim = contrastive_loss(z_cc, z_text, model.logit_scale)\n","            loss_mlo_text, mlo_text_summed_cos_sim = contrastive_loss(z_mlo, z_text, model.logit_scale)\n","            loss_img_img, cc_mlo_summed_cos_sim = contrastive_loss(z_cc, z_mlo, model.logit_scale)\n","\n","            # Recording the output to avoid recomputation for evaluation purpose\n","            all_training_z_cc.append(z_cc.float().cpu())\n","            all_training_z_mlo.append(z_mlo.float().cpu())\n","            all_training_z_text.append(z_text.float().cpu())\n","            cc_text_total_cos_sim += cc_text_summed_cos_sim.item()\n","            mlo_text_total_cos_sim += mlo_text_summed_cos_sim.item()\n","            cc_mlo_total_cos_sim += cc_mlo_summed_cos_sim.item()\n","\n","            # Weighted Sum (Can tune these weights)\n","            # total_loss = (loss_cc_text + loss_mlo_text + loss_img_img) / 3 # Original MVS-CLIP loss setup\n","            total_loss = (loss_cc_text + loss_mlo_text) / 2 # Removed MVS loss for ablation study\n","\n","            total_loss.backward()\n","\n","            # Back propagation\n","            optimizer.step()\n","            lr_scheduler.step() # Update learning rate\n","            optimizer.zero_grad()\n","\n","            epoch_loss += total_loss.item()\n","            progress_bar.set_postfix({\"loss\": epoch_loss / (step + 1)})\n","\n","        # Compute performance metrics for logging\n","        cc_text_cos_sim = cc_text_total_cos_sim / training_data_size\n","        mlo_text_cos_sim = mlo_text_total_cos_sim / training_data_size\n","        cc_mlo_cos_sim = cc_mlo_total_cos_sim / training_data_size\n","\n","        z_cc_training = torch.cat(all_training_z_cc, dim=0)\n","        z_mlo_training = torch.cat(all_training_z_mlo, dim=0)\n","        z_text_training = torch.cat(all_training_z_text, dim=0)\n","\n","        cc_text_training_metrics = compute_recall_k(z_cc_training, z_text_training)\n","        mlo_text_training_metrics = compute_recall_k(z_mlo_training, z_text_training)\n","        cc_mlo_training_metrics = compute_recall_k(z_cc_training, z_mlo_training)\n","        # ========== Training loop ==========\n","\n","\n","        # ========== Evaluation loop ==========\n","        # Evaluation in terms of loss, similarity metrics, R@1,2,3 performance\n","        model.eval()    # Switch to evaluation mode\n","\n","        evaluation_loss = 0\n","        all_evaluation_z_cc = []\n","        all_evaluation_z_mlo = []\n","        all_evaluation_z_text = []\n","        evaluation_cc_text_total_cos_sim = 0\n","        evaluation_mlo_text_total_cos_sim = 0\n","        evaluation_cc_mlo_total_cos_sim = 0\n","\n","        with torch.no_grad():\n","            for batch in validation_dataloader:\n","                # Move inputs to GPU\n","                pv_cc = batch[\"pixel_values_cc\"].cuda().to(torch.bfloat16)\n","                pv_mlo = batch[\"pixel_values_mlo\"].cuda().to(torch.bfloat16)\n","                ids = batch[\"input_ids\"].cuda()\n","                mask = batch[\"attention_mask\"].cuda()\n","\n","                # Forward\n","                z_cc, z_mlo, z_text = model(pv_cc, pv_mlo, ids, mask)\n","\n","                # Calculate Losses\n","                loss_cc_text, cc_text_summed_cos_sim = contrastive_loss(z_cc, z_text, model.logit_scale)\n","                loss_mlo_text, mlo_text_summed_cos_sim = contrastive_loss(z_mlo, z_text, model.logit_scale)\n","                loss_img_img, cc_mlo_summed_cos_sim = contrastive_loss(z_cc, z_mlo, model.logit_scale)\n","                total_loss = (loss_cc_text + loss_mlo_text + loss_img_img) / 3\n","\n","                # Recording the output to avoid recomputation for evaluation purpose\n","                all_evaluation_z_cc.append(z_cc.float().cpu())\n","                all_evaluation_z_mlo.append(z_mlo.float().cpu())\n","                all_evaluation_z_text.append(z_text.float().cpu())\n","                evaluation_cc_text_total_cos_sim += cc_text_summed_cos_sim.item()\n","                evaluation_mlo_text_total_cos_sim += mlo_text_summed_cos_sim.item()\n","                evaluation_cc_mlo_total_cos_sim += cc_mlo_summed_cos_sim.item()\n","\n","                evaluation_loss += total_loss.item()\n","\n","        # Compute performance metrics for logging\n","        evaluation_cc_text_cos_sim = evaluation_cc_text_total_cos_sim / validation_data_size\n","        evaluation_mlo_text_cos_sim = evaluation_mlo_text_total_cos_sim / validation_data_size\n","        evaluation_cc_mlo_cos_sim = evaluation_cc_mlo_total_cos_sim / validation_data_size\n","\n","        z_cc_evaluation = torch.cat(all_evaluation_z_cc, dim=0)\n","        z_mlo_evaluation = torch.cat(all_evaluation_z_mlo, dim=0)\n","        z_text_evaluation = torch.cat(all_evaluation_z_text, dim=0)\n","\n","        cc_text_evaluation_metrics = compute_recall_k(z_cc_evaluation, z_text_evaluation)\n","        mlo_text_evaluation_metrics = compute_recall_k(z_mlo_evaluation, z_text_evaluation)\n","        cc_mlo_evaluation_metrics = compute_recall_k(z_cc_evaluation, z_mlo_evaluation)\n","\n","        avg_eval_loss = evaluation_loss / len(validation_dataloader)\n","        # ========== Evaluation loop ==========\n","\n","\n","        # ========== Logging Step ==========\n","        logger.log({\"training/epoch_time_taken\": progress_bar.format_dict['elapsed'],\n","                    \"training/logit_scale\": model.logit_scale.exp().item(),\n","                    \"training/epoch_loss\": epoch_loss / num_steps,\n","                    \"training/cc_text_sim\":cc_text_cos_sim,\n","                    \"training/mlo_text_sim\":mlo_text_cos_sim,\n","                    \"training/cc_mlo_sim\":cc_mlo_cos_sim,\n","                    **{f\"training/cc-text-{k}\": v for k, v in cc_text_training_metrics.items()},\n","                    **{f\"training/mlo-text-{k}\": v for k, v in mlo_text_training_metrics.items()},\n","                    **{f\"training/cc-mlo-{k}\": v for k, v in cc_mlo_training_metrics.items()},\n","                    \"evaluation/epoch_loss\": avg_eval_loss,\n","                    \"evaluation/cc_text_sim\": evaluation_cc_text_cos_sim,\n","                    \"evaluation/mlo_text_sim\": evaluation_mlo_text_cos_sim,\n","                    \"evaluation/cc_mlo_sim\": evaluation_cc_mlo_cos_sim,\n","                    **{f\"evaluation/cc-text-{k}\": v for k, v in cc_text_evaluation_metrics.items()},\n","                    **{f\"evaluation/mlo-text-{k}\": v for k, v in mlo_text_evaluation_metrics.items()},\n","                    **{f\"evaluation/cc-mlo-{k}\": v for k, v in cc_mlo_evaluation_metrics.items()},\n","                    })\n","        # ========== Logging Step ==========\n","\n","\n","        # ========== Saving or Early Stopping ==========\n","        if avg_eval_loss < best_eval_loss: # Best evaluation performance found\n","            best_eval_loss = avg_eval_loss\n","            best_eval_loss_epoch = epoch+1\n","\n","            # Save best model\n","            save_path = os.path.join(Config.output_dir, \"checkpoint\")\n","            os.makedirs(save_path, exist_ok=True)\n","            # Save LoRA\n","            model.base_model.save_pretrained(save_path)\n","            # Save Projection Heads manually\n","            torch.save(model.visual_projection.state_dict(), os.path.join(save_path, \"visual_proj.pt\"))\n","            torch.save(model.text_projection.state_dict(), os.path.join(save_path, \"text_proj.pt\"))\n","            print(f\"Saved checkpoint to {save_path}\")\n","\n","        elif epoch+1 - best_eval_loss_epoch >= 3: # Early stopping if 3 consecutive epoch did not improve best eval loss\n","            print(f\"Early stopping at epoch {epoch+1} due to no improvement in evaluation loss\")\n","            break\n","        # ========== Saving or Early Stopping ==========\n","\n","    logger.finish()\n"],"metadata":{"id":"NU11mzJhOxGl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Result logged here\n","# https://wandb.ai/wjlingz-none/mammogram-contrastive-learning-project/"],"metadata":{"id":"D5zi_c67Pxfk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Run the training loop"],"metadata":{"id":"pSE1KNdtO4VB"}},{"cell_type":"code","source":["train()"],"metadata":{"id":"KDEvOrTjOsLV","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Save Model"],"metadata":{"id":"S5VkpRwrew8Q"}},{"cell_type":"code","source":["# Save to drive (Manually check what epoch is the latest)\n","!cp -r /content/mammo_clip_checkpoints/checkpoint /content/drive/MyDrive/\n"],"metadata":{"id":"-B12R1LWYHWu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import runtime\n","import time\n","\n","print(\"‚è≥ Waiting 30s for background tasks...\")\n","time.sleep(30)\n","\n","print(\"üëã Disconnecting runtime.\")\n","runtime.unassign()"],"metadata":{"id":"MCY8sBvwUcJ7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Reload back the model from drive then upload to HuggingFace Hub"],"metadata":{"id":"YVRfGkrwYnlF"}},{"cell_type":"markdown","source":["### Require disconnect and reconnect run time if not enough RAM\n","Remember to run the cell for Drive Mounting, Install and Import library, load dataset, Dataset definition, Model Architecture definition, Loss Function, Validation Function.  \n","\n","To verify validation performance, check appendix\n"],"metadata":{"id":"qKwUiLoQYsnd"}},{"cell_type":"code","source":["# import gc\n","\n","# del model\n","# torch.cuda.empty_cache()\n","# gc.collect()"],"metadata":{"id":"-PHie3ALaSyB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["checkpoint_dir = \"/content/drive/MyDrive/checkpoint\""],"metadata":{"id":"UuUTypIqk7HB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from peft import PeftModel, PeftConfig\n","import os\n","\n","# 1. Load Base Model (Dequantized) + Processor\n","base_model_id = \"google/medgemma-4b-it\"\n","\n","model_kwargs = dict(\n","    attn_implementation=\"sdpa\", # \"sdpa\", \"eager\"\n","    torch_dtype=torch.bfloat16,\n","    device_map=\"auto\",\n",")\n","\n","\n","dequantized_gemma = AutoModelForImageTextToText.from_pretrained(base_model_id, **model_kwargs)\n","\n","# 2. Load LoRA adapters from your hub repo\n","base_model = PeftModel.from_pretrained(dequantized_gemma, \"/content/drive/MyDrive/checkpoint\")\n","merged_base_model = base_model.merge_and_unload()\n","\n","processor = AutoProcessor.from_pretrained(base_model_id)\n","processor.tokenizer.padding_side = \"right\"\n","\n","## Upload to Hub\n","# Save to HuggingFace\n","from huggingface_hub import login\n","from google.colab import userdata\n","from huggingface_hub import HfApi\n","from huggingface_hub import hf_hub_download\n","\n","# 3. Login\n","login(token=userdata.get('HF_TOKEN'))\n","# FILL IN THE PROPER REPO_ID, COMMENTED OUT TO AVOID ACCIDENT\n","# \"weijietling/medgemma-4b-it-contrastive-trained-130126\" this model is the default contrastive training setup, early stopping at epoch 10, other model is for ablation study\n","# \"weijietling/medgemma-4b-it-contrastive-trained-150126-mvs-ablation\" this model use default clip contrastive setup without mvs loss (no img-img-loss)\n","repo_id = \"weijietling/medgemma-4b-it-contrastive-trained-150126-mvs-ablation\"\n","\n","# 4. Push the merged model (called contrastive model in the future)\n","merged_base_model.push_to_hub(repo_id) # Takes ~10 minutes\n","\n","# 5. Push the tokenizer/processor so you can use it later\n","processor.push_to_hub(repo_id)\n","\n","# 6. Upload them to the Hub\n","api = HfApi()\n","\n","drive_visual_path = os.path.join(checkpoint_dir, \"visual_proj.pt\")\n","drive_text_path = os.path.join(checkpoint_dir, \"text_proj.pt\")\n","\n","# Upload Visual Projection\n","api.upload_file(\n","    path_or_fileobj=drive_visual_path,    # Source: Your Drive file\n","    path_in_repo=\"visual_projection.pt\",  # Destination: Name in HF Hub\n","    repo_id=repo_id,\n","    repo_type=\"model\"\n",")\n","\n","# Upload Text Projection\n","api.upload_file(\n","    path_or_fileobj=drive_text_path,      # Source: Your Drive file\n","    path_in_repo=\"text_projection.pt\",    # Destination: Name in HF Hub\n","    repo_id=repo_id,\n","    repo_type=\"model\"\n",")\n","\n","print(f\"Successfully pushed all files to https://huggingface.co/{repo_id}\")"],"metadata":{"id":"cY2Y6ValcGaz","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Comparison:\n","1. Medgemma-4b-it + few-shot training\n","2. Medgemma-4b-it + contrastive training + few-shot training\n","3. Medgemma-4b-it + image-report pretraining\n","4. Medgemma-4b-it + contrastive training + image-report pretraining\n"],"metadata":{"id":"oo-VU60_Yew-"}},{"cell_type":"markdown","source":["# Appendix"],"metadata":{"id":"PAdjz4dbYJ-p"}},{"cell_type":"markdown","source":["## Load model\n"],"metadata":{"id":"3BZ6ESULxFQR"}},{"cell_type":"markdown","source":["### Quantized Gemma + LoRA Adapter"],"metadata":{"id":"Twko3nAqxffg"}},{"cell_type":"code","source":["# Test for model weight loading\n","## Ensure the performance on validation set is same as during training time (validation set)\n","\n","from peft import PeftModel, PeftConfig\n","\n","# 1. Setup Config\n","class Config:\n","    model_id = \"google/medgemma-4b-it\"\n","    validation_file = \"validation_dataset.jsonl\"\n","    num_workers = 2\n","    max_length = 128\n","\n","# 2. Load Base Model (QLoRA)\n","model_kwargs = dict(\n","    attn_implementation=\"sdpa\",\n","    torch_dtype=torch.bfloat16,\n","    device_map=\"auto\",\n",")\n","\n","# Skip quantization if we are going to merge the LoRA adapter\n","model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=model_kwargs[\"torch_dtype\"],\n","    bnb_4bit_quant_storage=model_kwargs[\"torch_dtype\"],\n",")\n","\n","gemma_model = AutoModelForImageTextToText.from_pretrained(Config.model_id, **model_kwargs)\n","\n","# 3. Load Processor and fix padding\n","processor = AutoProcessor.from_pretrained(Config.model_id)\n","processor.tokenizer.padding_side = \"right\"\n","\n","# 4. Load LoRA Adapters\n","# Point to your saved checkpoint folder\n","checkpoint_path = \"/content/drive/MyDrive/checkpoint-epoch-10\"\n","base_model = PeftModel.from_pretrained(gemma_model, checkpoint_path)\n","\n","# 5. Initialize Wrapper\n","# Note: We must ensure the wrapper is on the correct device/dtype\n","model = MammoContrastiveModel(base_model, hidden_size=base_model.config.text_config.hidden_size).cuda()\n","\n","# 6. Load Projection Heads\n","visual_proj_path = os.path.join(checkpoint_path, \"visual_proj.pt\")\n","text_proj_path = os.path.join(checkpoint_path, \"text_proj.pt\")\n","\n","model.visual_projection.load_state_dict(torch.load(visual_proj_path))\n","model.text_projection.load_state_dict(torch.load(text_proj_path))\n","\n","# 7. Manually Restore Logit Scale (Optional but recommended for Loss consistency)\n","# Since you didn't save it, check your WandB logs for the value of 'training/logit_scale'\n","# at epoch 10. Let's say it was 4.2.\n","# model.logit_scale.data = torch.tensor(4.2).float().cuda()\n","\n","print(\"Model fully restored!\")"],"metadata":{"collapsed":true,"id":"I9ZJeuSeJ3kA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Dequantized Gemma Merged with LoRA Adapter\n","(Quantized Gemma Merged with LoRA gives sucky performance)"],"metadata":{"id":"fAdZQXKfxcCM"}},{"cell_type":"code","source":["# Test for model weight loading\n","## Ensure the performance on validation set is same as during training time (validation set)\n","\n","from peft import PeftModel, PeftConfig\n","\n","# 1. Setup Config\n","class Config:\n","    model_id = \"google/medgemma-4b-it\"\n","    validation_file = \"validation_dataset.jsonl\"\n","    num_workers = 2\n","    max_length = 128\n","\n","# 2. Load Base Model (QLoRA)\n","model_kwargs = dict(\n","    attn_implementation=\"sdpa\",\n","    torch_dtype=torch.bfloat16,\n","    device_map=\"auto\",\n",")\n","\n","# Skip quantization if we are going to merge the LoRA adapter\n","# model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n","#     load_in_4bit=True,\n","#     bnb_4bit_use_double_quant=True,\n","#     bnb_4bit_quant_type=\"nf4\",\n","#     bnb_4bit_compute_dtype=model_kwargs[\"torch_dtype\"],\n","#     bnb_4bit_quant_storage=model_kwargs[\"torch_dtype\"],\n","# )\n","\n","gemma_model = AutoModelForImageTextToText.from_pretrained(Config.model_id, **model_kwargs)\n","\n","# 3. Load Processor and fix padding\n","processor = AutoProcessor.from_pretrained(Config.model_id)\n","processor.tokenizer.padding_side = \"right\"\n","\n","# 4. Load LoRA Adapters\n","# Point to your saved checkpoint folder\n","checkpoint_path = \"/content/drive/MyDrive/checkpoint-epoch-10\"\n","base_model = PeftModel.from_pretrained(gemma_model, checkpoint_path)\n","\n","# 5. Initialize Wrapper\n","# Note: We must ensure the wrapper is on the correct device/dtype\n","merged_base_model = base_model.merge_and_unload()\n","model = MammoContrastiveModel(merged_base_model, hidden_size=base_model.config.text_config.hidden_size).cuda()\n","\n","# 6. Load Projection Heads\n","visual_proj_path = os.path.join(checkpoint_path, \"visual_proj.pt\")\n","text_proj_path = os.path.join(checkpoint_path, \"text_proj.pt\")\n","\n","model.visual_projection.load_state_dict(torch.load(visual_proj_path))\n","model.text_projection.load_state_dict(torch.load(text_proj_path))\n","\n","# 7. Manually Restore Logit Scale (Optional but recommended for Loss consistency)\n","# Since you didn't save it, check your WandB logs for the value of 'training/logit_scale'\n","# at epoch 10. Let's say it was 4.2.\n","# model.logit_scale.data = torch.tensor(4.2).float().cuda()\n","\n","print(\"Model fully restored!\")"],"metadata":{"id":"uIQ7aWSKxcRU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Quantized Contrastive Model (HuggingFace Hub)\n","(Contrastive Model = Dequantized Gemma Merged with LoRA)"],"metadata":{"id":"gnjgiUMVx4-Z"}},{"cell_type":"code","source":["# Test for model weight loading THIS TIME LOAD THE MERGED MODEL STORED IN HUGGINGFACE HUB\n","## Ensure the performance on validation set is same as during training time (validation set)\n","\n","from peft import PeftModel, PeftConfig\n","from huggingface_hub import login\n","from google.colab import userdata\n","from huggingface_hub import HfApi\n","from huggingface_hub import hf_hub_download\n","\n","# Replace with your WRITE token\n","login(token=userdata.get('HF_TOKEN'))\n","\n","# 1. Setup Config\n","class Config:\n","    model_id = \"weijietling/medgemma-4b-it-contrastive-trained-150126-mvs-ablation\" # \"weijietling/medgemma-4b-it-contrastive-trained-130126\"\n","    validation_file = \"validation_dataset.jsonl\"\n","    num_workers = 2\n","    max_length = 128\n","\n","# 2. Load Base Model (QLoRA)\n","model_kwargs = dict(\n","    attn_implementation=\"sdpa\",\n","    torch_dtype=torch.bfloat16,\n","    device_map=\"auto\",\n",")\n","\n","# Skip quantization if we are going to merge the LoRA adapter\n","model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_use_double_quant=True,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_compute_dtype=model_kwargs[\"torch_dtype\"],\n","    bnb_4bit_quant_storage=model_kwargs[\"torch_dtype\"],\n",")\n","\n","contrastive_model = AutoModelForImageTextToText.from_pretrained(Config.model_id, **model_kwargs)\n","\n","# 3. Load Processor and fix padding\n","processor = AutoProcessor.from_pretrained(Config.model_id)\n","processor.tokenizer.padding_side = \"right\"\n","\n","# 4. Load LoRA Adapters\n","# Point to your saved checkpoint folder\n","# checkpoint_path = \"/content/drive/MyDrive/checkpoint-epoch-10\"\n","# base_model = PeftModel.from_pretrained(gemma_model, checkpoint_path)\n","\n","# 5. Initialize Wrapper\n","# Note: We must ensure the wrapper is on the correct device/dtype\n","# model = MammoContrastiveModel(base_model, hidden_size=base_model.config.text_config.hidden_size).cuda()\n","\n","model = MammoContrastiveModel(contrastive_model, hidden_size=contrastive_model.config.text_config.hidden_size).cuda()\n","\n","# 6. Load Projection Heads\n","visual_proj_path = hf_hub_download(repo_id=Config.model_id, filename=\"visual_projection.pt\", repo_type=\"model\")\n","text_proj_path = hf_hub_download(repo_id=Config.model_id, filename=\"text_projection.pt\", repo_type=\"model\")\n","\n","model.visual_projection.load_state_dict(torch.load(visual_proj_path))\n","model.text_projection.load_state_dict(torch.load(text_proj_path))\n","\n","# 7. Manually Restore Logit Scale (Optional but recommended for Loss consistency)\n","# Since you didn't save it, check your WandB logs for the value of 'training/logit_scale'\n","# at epoch 10. Let's say it was 4.2.\n","# model.logit_scale.data = torch.tensor(4.2).float().cuda()\n","\n","print(\"Model fully restored!\")"],"metadata":{"id":"Vd9ehZ91qNgG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Evaluation on validation"],"metadata":{"id":"GqCE4DlUyJzE"}},{"cell_type":"code","source":["# Evaluation on the whole validation set\n","validation_dataset = MammoDataset(Config.validation_file, processor, split=\"test\")\n","validation_dataloader = DataLoader(validation_dataset, batch_size=4, shuffle=False, num_workers=Config.num_workers)\n","\n","model.eval()    # Switch to evaluation mode\n","\n","evaluation_loss = 0\n","all_evaluation_z_cc = []\n","all_evaluation_z_mlo = []\n","all_evaluation_z_text = []\n","evaluation_cc_text_total_cos_sim = 0\n","evaluation_mlo_text_total_cos_sim = 0\n","evaluation_cc_mlo_total_cos_sim = 0\n","validation_data_size = len(validation_dataset)\n","\n","with torch.no_grad():\n","    for batch in validation_dataloader:\n","        # Move inputs to GPU\n","        pv_cc = batch[\"pixel_values_cc\"].cuda().to(torch.bfloat16)\n","        pv_mlo = batch[\"pixel_values_mlo\"].cuda().to(torch.bfloat16)\n","        ids = batch[\"input_ids\"].cuda()\n","        mask = batch[\"attention_mask\"].cuda()\n","\n","        # Forward\n","        z_cc, z_mlo, z_text = model(pv_cc, pv_mlo, ids, mask)\n","\n","        # Calculate Losses\n","        loss_cc_text, cc_text_summed_cos_sim = contrastive_loss(z_cc, z_text, model.logit_scale)\n","        loss_mlo_text, mlo_text_summed_cos_sim = contrastive_loss(z_mlo, z_text, model.logit_scale)\n","        loss_img_img, cc_mlo_summed_cos_sim = contrastive_loss(z_cc, z_mlo, model.logit_scale)\n","        total_loss = (loss_cc_text + loss_mlo_text + loss_img_img) / 3\n","\n","        # Recording the output to avoid recomputation for evaluation purpose\n","        all_evaluation_z_cc.append(z_cc.float().cpu())\n","        all_evaluation_z_mlo.append(z_mlo.float().cpu())\n","        all_evaluation_z_text.append(z_text.float().cpu())\n","        evaluation_cc_text_total_cos_sim += cc_text_summed_cos_sim.item()\n","        evaluation_mlo_text_total_cos_sim += mlo_text_summed_cos_sim.item()\n","        evaluation_cc_mlo_total_cos_sim += cc_mlo_summed_cos_sim.item()\n","\n","        evaluation_loss += total_loss.item()\n","\n","# Compute performance metrics for logging\n","evaluation_cc_text_cos_sim = evaluation_cc_text_total_cos_sim / validation_data_size\n","evaluation_mlo_text_cos_sim = evaluation_mlo_text_total_cos_sim / validation_data_size\n","evaluation_cc_mlo_cos_sim = evaluation_cc_mlo_total_cos_sim / validation_data_size\n","\n","z_cc_evaluation = torch.cat(all_evaluation_z_cc, dim=0)\n","z_mlo_evaluation = torch.cat(all_evaluation_z_mlo, dim=0)\n","z_text_evaluation = torch.cat(all_evaluation_z_text, dim=0)\n","\n","cc_text_evaluation_metrics = compute_recall_k(z_cc_evaluation, z_text_evaluation)\n","mlo_text_evaluation_metrics = compute_recall_k(z_mlo_evaluation, z_text_evaluation)\n","cc_mlo_evaluation_metrics = compute_recall_k(z_cc_evaluation, z_mlo_evaluation)\n","\n","avg_eval_loss = evaluation_loss / len(validation_dataloader)\n","\n","print(evaluation_cc_text_cos_sim)\n","print(evaluation_mlo_text_cos_sim)\n","print(evaluation_cc_mlo_cos_sim)\n","print(cc_text_evaluation_metrics)\n","print(mlo_text_evaluation_metrics)\n","print(cc_mlo_evaluation_metrics)\n","print(avg_eval_loss)"],"metadata":{"id":"3n73zbxZRki6","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Quantized Contrastive Model (Ablated -- no img_img_loss)  \n","0.3625710227272727  \n","0.3938210227272727  \n","0.7333096590909091  \n","{'R@1': 25.0, 'R@2': 34.090909361839294, 'R@3': 43.18181872367859}  \n","{'R@1': 27.272728085517883, 'R@2': 34.090909361839294, 'R@3': 43.18181872367859}  \n","{'R@1': 77.27272510528564, 'R@2': 84.09090638160706, 'R@3': 86.36363744735718}  \n","0.6269891912286932  "],"metadata":{"id":"Tw9kvVb2n4wx"}},{"cell_type":"markdown","source":["Quantized Gemma Merged with LoRA Adapter:  (Dequantized first, then merge, then quantized again)  \n","0.24884588068181818  \n","0.26171875  \n","0.7432528409090909  \n","{'R@1': 18.18181872367859, 'R@2': 20.454545319080353, 'R@3': 29.545453190803528}  \n","{'R@1': 22.727273404598236, 'R@2': 31.81818127632141, 'R@3': 38.63636255264282}  \n","{'R@1': 88.63636255264282, 'R@2': 95.45454382896423, 'R@3': 95.45454382896423}  \n","0.6533203125  "],"metadata":{"id":"LXM0tgwBvqwl"}},{"cell_type":"markdown","source":["Dequantized Gemma Merged with LoRA Adapter:   (Dequantized first, then merge)  \n","0.21075994318181818  \n","0.21484375  \n","0.7681107954545454  \n","{'R@1': 18.18181872367859, 'R@2': 29.545453190803528, 'R@3': 34.090909361839294}  \n","{'R@1': 18.18181872367859, 'R@2': 36.36363744735718, 'R@3': 40.909090638160706}  \n","{'R@1': 88.63636255264282, 'R@2': 93.18181872367859, 'R@3': 95.45454382896423}  \n","0.6944691051136364  "],"metadata":{"id":"42zvD-kCvUHE"}},{"cell_type":"markdown","source":["Quantized Gemma + LoRA Adapter:  (Dequantized and load LoRA without merging)  \n","0.26118607954545453  \n","0.2731711647727273  \n","0.7386363636363636  \n","{'R@1': 18.18181872367859, 'R@2': 27.272728085517883, 'R@3': 31.81818127632141}  \n","{'R@1': 25.0, 'R@2': 34.090909361839294, 'R@3': 43.18181872367859}  \n","{'R@1': 84.09090638160706, 'R@2': 93.18181872367859, 'R@3': 95.45454382896423}  \n","0.6402254971590909  "],"metadata":{"id":"RZTydsVvfntQ"}},{"cell_type":"markdown","source":["## Others"],"metadata":{"id":"biftkfM5yMTS"}},{"cell_type":"code","source":["# Uses 24GB of ram using config batch size 4, grad accumulation 1. 36gb after adding requires_grad_(True)\n","# Batch size, grad acc step, RAM, time trainning per epoch, initial loss, loss at the end of first epoch, loss at the beginning of second epoch\n","# 4, 1, 35.9gb, 10min40sec, 1.4, 1.18, 0.88 (forgot to activate image augmentation pipeline)\n","# 4, 8, 35.9gb, 10min42sec, 1.4, 1.37, 1.29 (forgot to activate image augmentation pipeline) -> probably dont use gradient accumulation, no benefits using it\n","# 4, 1, 35.9gb, 10min53sec, 1.4 , 1.2, 0.9 (with augmentation from here onward) -> can use higher batch size probably\n","# 8, 1, 55.9gb(63.9), 10min07sec, 2.08, 1.81(1.17)(0.816)(0.584), 1.14(0.82)(0.64)(0.535)\n","# (previously all using \"eager\" attention, now trying \"sdpa\")\n","# (accidentally used dataloader instead of validation_dataloader) 8, 1, 20.4gb (28.8 if add validation loop), 6min50sec (3min11sec for validation loop), 1.81, 1.3\n","# (correctly using validation_dataloader) 8, 1, 20.4gb (20.4gb if add validation loop), 6min50sec (33sec for validation loop, 10sec in subsequent validation), 1.81(1.2)(0.84)(0.59), 1.3 -> sdpa have similar performance with eager. validation is working correctly\n","# 32, 1, 44.0gb, 8min4sec (1min02sec validation loop), 3.34"],"metadata":{"id":"0a43Rue50aZf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# To find the best ratio range for data augmentation so that the original mammography dimension (tall rectangular) does not deviate too much after augmentation\n","import os\n","from PIL import Image\n","import numpy as np\n","\n","def analyze_image_ratios(folder_path):\n","    ratios = []\n","\n","    for filename in os.listdir(folder_path):\n","        if filename.lower().endswith(('.jpg', '.jpeg', '.png', '.bmp', '.gif')):\n","            img_path = os.path.join(folder_path, filename)\n","            try:\n","                img = Image.open(img_path)\n","                width, height = img.size\n","                ratio = width / height\n","                ratios.append(ratio)\n","            except Exception as e:\n","                print(f\"Error processing {filename}: {e}\")\n","\n","    ratios = np.array(ratios)\n","    avg_ratio = np.mean(ratios)\n","    std_ratio = np.std(ratios)\n","\n","    return avg_ratio, std_ratio, ratios\n","\n","# Usage\n","folder_path = \"/content/drive/MyDrive/CDD-CESM-curated-dataset/images/\"\n","avg, std, all_ratios = analyze_image_ratios(folder_path)\n","\n","print(f\"Average ratio (W/H): {avg:.4f}\")\n","print(f\"Standard deviation: {std:.4f}\")\n","print(f\"Total images: {len(all_ratios)}\")"],"metadata":{"id":"_O9E7FU7pQKk","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["example_image_path = data['train'][0]['cc_path']"],"metadata":{"id":"iefnxdDnf-l6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","from matplotlib.patches import ConnectionPatch\n","from PIL import Image\n","import torch\n","import torchvision.transforms as T\n","import torchvision.transforms.functional as TF\n","import numpy as np\n","\n","# 1. SETUP: Define your image path\n","example_image_path = data['train'][0]['cc_path']\n","# Load image (or create dummy if file missing for testing)\n","try:\n","    orig_img = Image.open(example_image_path).convert('RGB')\n","except (FileNotFoundError, OSError):\n","    print(f\"File '{example_image_path}' not found. Generating a random 2000x2000 image for demonstration.\")\n","    orig_img = Image.fromarray(np.uint8(np.random.rand(2000, 2000, 3) * 255))\n","\n","# ---------------------------------------------------------\n","# PIPELINE EXECUTION (Step-by-Step)\n","# ---------------------------------------------------------\n","\n","# 1. Calculate Crop Params (No resize yet)\n","# We simulate a crop that is 70%-90% of the original image\n","roi_selector = T.RandomResizedCrop(size=(896, 896), scale=(0.7, 0.7), ratio=(0.45, 0.65))\n","i, j, h, w = roi_selector.get_params(orig_img, scale=(0.7, 0.7), ratio=(0.45, 0.65))\n","\n","# 2. Perform Crop\n","step1_crop = TF.crop(orig_img, i, j, h, w)\n","\n","# 3. Perform Jitter\n","jitter_transform = T.ColorJitter(brightness=0.4, contrast=0.4)\n","step2_jitter = jitter_transform(step1_crop)\n","\n","# 4. Perform Blur\n","blur_transform = T.GaussianBlur(kernel_size=39, sigma=2.0)\n","step3_blur = blur_transform(step2_jitter)\n","\n","# 5. Perform Resize (Final 896x896)\n","step4_resize = TF.resize(step3_blur, size=(896, 896))\n","\n","# ---------------------------------------------------------\n","# PLOTTING WITH ARROWS & RESIZING\n","# ---------------------------------------------------------\n","\n","# We use width_ratios to make the last column (Resize) visually smaller (0.6 width)\n","# vs the others (1.0 width).\n","fig, axs = plt.subplots(1, 5, figsize=(24, 6),\n","                        gridspec_kw={'width_ratios': [1, 1, 1, 1, 0.6]})\n","\n","# --- Plot 1: Original with Box ---\n","axs[0].imshow(orig_img)\n","axs[0].set_title(f\"1. Original Input\\n{orig_img.size}\", fontsize=22, fontweight='bold')\n","# Draw the red box indicating what will be cropped\n","rect = patches.Rectangle((j, i), w, h, linewidth=4, edgecolor='#ff0055', facecolor='none')\n","axs[0].add_patch(rect)\n","axs[0].axis('off')\n","\n","# --- Plot 2: Cropped Patch ---\n","axs[1].imshow(step1_crop)\n","axs[1].set_title(f\"2. Cropped Patch\\n{step1_crop.size}\", fontsize=22, fontweight='bold')\n","axs[1].axis('off')\n","\n","# --- Plot 3: Jittered ---\n","axs[2].imshow(step2_jitter)\n","axs[2].set_title(\"3. Color Jitter\", fontsize=22, fontweight='bold')\n","axs[2].axis('off')\n","\n","# --- Plot 4: Blurred ---\n","axs[3].imshow(step3_blur)\n","axs[3].set_title(\"4. Gaussian Blur\", fontsize=22, fontweight='bold')\n","axs[3].axis('off')\n","\n","# --- Plot 5: Final Resize (Will appear smaller due to width_ratios) ---\n","axs[4].imshow(step4_resize)\n","axs[4].set_title(f\"5. Resized Final\\n{step4_resize.size}\", fontsize=22, fontweight='bold')\n","axs[4].axis('off')\n","\n","# ---------------------------------------------------------\n","# DRAWING ARROWS\n","# ---------------------------------------------------------\n","def draw_arrow(ax_src, ax_dst):\n","    # Create an arrow extending from the right of source to left of dest\n","    con = ConnectionPatch(xyA=(1.0, 0.5), xyB=(0.0, 0.5),\n","                          coordsA=\"axes fraction\", coordsB=\"axes fraction\",\n","                          axesA=ax_src, axesB=ax_dst,\n","                          arrowstyle=\"simple,head_width=2,head_length=2\", linewidth=2, color=\"black\",\n","                          shrinkA=10, shrinkB=10) # shrink adds padding so arrow doesn't touch image\n","    ax_src.add_artist(con)\n","\n","# Draw arrows between 0->1, 1->2, 2->3, 3->4\n","for k in range(4):\n","    draw_arrow(axs[k], axs[k+1])\n","\n","plt.tight_layout()\n","plt.show()\n","\n","# Optional: Save to file\n","# plt.savefig('pipeline_flowchart.png', dpi=300, bbox_inches='tight')"],"metadata":{"id":"rhEVrBR_dGvv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.savefig('pipeline_flowchart.png', dpi=900, bbox_inches='tight')"],"metadata":{"id":"qobhmiufdfWi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data['train'][1]"],"metadata":{"id":"VT08gePQZ2sK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"79JQI-EyZ3oN"},"execution_count":null,"outputs":[]}]}